{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utilities import fetch_data, live_plot\n",
    "from utils.td3 import Agent, DeepAgent, RobustAgent\n",
    "from utils.envs import TradingEnv1, TradingEnv4, TradingEnv5, TradingEnv6, TradingEnv7, TradingEnv9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'TradingEnv9'\n",
    "db_name = './data/HistoricalPriceData.db'\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snfGHM_8rAnv"
   },
   "source": [
    "##### Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_data(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include 2020 shock\n",
    "for tick in data:\n",
    "    data[tick] = data[tick][\n",
    "        data[tick]['date'] <= pd.to_datetime('2020-02-01')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 520 # About two years\n",
    "idx = np.random.randint(0, len(data['fb'])-size)\n",
    "\n",
    "for t in data:\n",
    "    data[t] = data[t].iloc[idx: idx+size]\n",
    "    data[t].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snfGHM_8rAnv"
   },
   "source": [
    "##### Initialize environment and set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-xn7OWBrPP8",
    "outputId": "ac94c862-ca63-4ff6-c338-f4f65a4c91b2"
   },
   "outputs": [],
   "source": [
    "env = eval(f'{env_name}(data)')\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = np.prod(env.observation_space.shape)\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg1ISVQGqObn"
   },
   "source": [
    "##### Create folder in which trained models will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOSuBYdsqVMX"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "if not os.path.exists('./models/all_td3'):\n",
    "    os.makedirs('./models/all_td3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAs1OaNYp5ht",
    "outputId": "d22ab539-f287-4856-c7a9-f627db21d77d"
   },
   "outputs": [],
   "source": [
    "file_name = f'TD3_{env_name}_{seed}'\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize agent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1e6\n",
    "batch_size = 100\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 5e-3\n",
    "policy_freq = 2\n",
    "lr = 1e-3\n",
    "\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "expl_noise = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaaTcOGKsLyh",
    "outputId": "53a75b83-e251-496b-c21e-64c7d45fd8f0"
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    state_dim, \n",
    "    action_dim, \n",
    "    max_action,\n",
    "    eta=lr,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXNQHlxItDHz"
   },
   "source": [
    "##### Initialize training variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load(file_name, './models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_step = 3e4 # Number of random steps at start\n",
    "\n",
    "episodes = 1e3 # Number of episodes to train on\n",
    "total_steps = 0\n",
    "training = False\n",
    "report = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = env.positions\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_trace = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in np.arange(1, episodes+1):             \n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    steps = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    while not done:        \n",
    "        \n",
    "        if total_steps == starting_step:\n",
    "            print('Begin training')\n",
    "            training = True\n",
    "            \n",
    "        if not training:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(np.array(obs))\n",
    "            \n",
    "            if expl_noise != 0:\n",
    "                noise = np.random.normal(0, expl_noise, size=env.action_space.shape[0])\n",
    "                action = (action+noise).clip(env.action_space.low, env.action_space.high)\n",
    "                \n",
    "        action_fmt = env.format_action(positions, action)\n",
    "                \n",
    "        new_obs, reward, done, info = env.step(action_fmt)\n",
    "        \n",
    "        episode_reward.append(reward)\n",
    "        agent.replay_buffer.add((obs, new_obs, action, reward, int(done)))\n",
    "                \n",
    "        obs = new_obs\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "\n",
    "        \n",
    "    agent.train(steps, batch_size, gamma, tau, policy_noise, noise_clip, policy_freq)\n",
    "    \n",
    "    reward_trace['net_worth_diff'].append(sum(np.array(env.net_worth)-np.array(env.net_worth_long)))\n",
    "    reward_trace['avg_episode_reward'].append(np.mean(episode_reward))\n",
    "    reward_trace['sum_episode_reward'].append(sum(episode_reward))\n",
    "    reward_trace['exploration_noise'].append(round(expl_noise, 9))\n",
    "    \n",
    "    ax = live_plot(reward_trace, trace='avg')                                                      ###\n",
    "    plt.show()                                                                                     ###\n",
    "    \n",
    "    if episode % report == 0:\n",
    "#        clear_output(wait=True)                                                                   ###\n",
    "        \n",
    "        print('Episode:', episode)\n",
    "        print('Average net worth diff', np.mean(reward_trace['net_worth_diff'][-report:]))\n",
    "        print('Average score:', np.mean(reward_trace['avg_episode_reward'][-report:]))\n",
    "        print('Exploration noise:', reward_trace['exploration_noise'][-1])\n",
    "        print('Training:', training)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        agent.save(\n",
    "            f'{file_name}_{int(episode)}', \n",
    "            './models/all_td3',\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot reward trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwd = pd.Series(reward_trace['net_worth_diff'])\n",
    "nwd = nwd.rolling(window=report, center=True, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(reward_trace['net_worth_diff'], 'b', alpha=0.7)\n",
    "plt.plot(nwd, 'b')\n",
    "plt.axvline(950, c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(reward_trace['avg_episode_reward'], 'b')\n",
    "plt.axvline(950, c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(reward_trace['sum_episode_reward'], 'b')\n",
    "plt.axvline(950, c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save only model from selected point in training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 950\n",
    "\n",
    "agent.load(\n",
    "    f'{file_name}_{idx}', \n",
    "    './models/all_td3',\n",
    ")\n",
    "\n",
    "agent.save(\n",
    "    file_name, \n",
    "    './models',\n",
    ")\n",
    "\n",
    "for item in os.listdir('./models/all_td3'):\n",
    "    if file_name in str(item):\n",
    "        os.remove(os.path.join('./models/all_td3', item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwd = pd.Series(reward_trace['net_worth_diff'])\n",
    "nwd = nwd.rolling(window=report, center=True, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "ax1.plot(reward_trace['net_worth_diff'], 'b', alpha=0.7, label='net_worth_diff')\n",
    "ax1.plot(nwd, 'b')\n",
    "ax1.axvline(idx, c='r', alpha=0.3)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_title('Environment 9 - Base TD3 Model: `TD3_TradingEnv9_42`')\n",
    "ax1.set_ylim([-4e8,8e8])\n",
    "\n",
    "\n",
    "ax2.plot(reward_trace['avg_episode_reward'], 'b', label='avg_episode_reward')\n",
    "ax2.axvline(idx, c='r', alpha=0.3)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_ylim([-4, 8])\n",
    "\n",
    "ax3.plot(reward_trace['sum_episode_reward'], 'b', label='sum_episode_reward')\n",
    "ax3.axvline(idx, c='r', alpha=0.3)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_ylim([-4e3, 8e3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./assets/TD3_TradingEnv9_42.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
